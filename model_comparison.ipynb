{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (1.54.0)\n",
      "Requirement already satisfied: datasets in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: nltk in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: tqdm in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (4.66.6)\n",
      "Requirement already satisfied: transformers in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: accelerate in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: torch in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: click in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: psutil in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from accelerate) (6.1.0)\n",
      "Requirement already satisfied: networkx in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: certifi in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/rahulshrivastava/anaconda3/envs/text_to_code/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai datasets nltk pandas tqdm transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import openai\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'punkt' tokenizer is available.\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"'punkt' tokenizer is available.\")\n",
    "except LookupError:\n",
    "    print(\"'punkt' tokenizer is not available.\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    return device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the datasets\n",
    "def load_and_sample_datasets(fraction=0.1, dataset_name='codeparrot/xlcost-text-to-code'):\n",
    "    # Load datasets\n",
    "    datasets = []\n",
    "    if dataset_name == 'codeparrot/xlcost-text-to-code':\n",
    "        try:\n",
    "            code_x_glue_dataset = load_dataset(\"codeparrot/xlcost-text-to-code\", split='train')\n",
    "            #code_x_glue_dataset = load_dataset('code_x_glue_ct_code_to_text', 'python', split='train')\n",
    "            datasets.append(code_x_glue_dataset)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load xlcost-text-to-code dataset: {e}\")\n",
    "    elif dataset_name == 'codeparrot/apps':\n",
    "        try:\n",
    "            apps_dataset = load_dataset('codeparrot/apps', split='all', trust_remote_code=True)\n",
    "            datasets.append(apps_dataset)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load codeparrot/apps dataset: {e}\")\n",
    "    elif dataset_name == 'codeparrot/codeparrot-clean': \n",
    "        try:\n",
    "            codeparrot_clean_dataset = load_dataset('codeparrot/codeparrot-clean', split='train')\n",
    "            datasets.append(codeparrot_clean_dataset)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load codeparrot/codeparrot-clean dataset: {e}\")\n",
    "\n",
    "    if not datasets:\n",
    "        raise ValueError(\"No datasets were loaded successfully.\")\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = concatenate_datasets(datasets)\n",
    "\n",
    "    # Sample 10% of the data\n",
    "    num_samples = int(len(combined_dataset) * fraction)\n",
    "    sampled_dataset = combined_dataset.shuffle(seed=42).select(range(num_samples))\n",
    "    return sampled_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the tokenizer and model\n",
    "def load_model_and_tokenizer(model_name='Salesforce/codegen-350M-mono', sampled_dataset=None):\n",
    "    total_examples = 0\n",
    "    exact_matches = 0\n",
    "    bleu_scores = []\n",
    "    response_times = []\n",
    "    results = []\n",
    "    no_text_or_code = 0\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "\n",
    "    # Ensure pad_token_id is set\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(f\"Loaded model: {model_name}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(\"dataset name : \" + sampled_dataset['name'])\n",
    "    print(f\"Number of examples: {len(sampled_dataset['data'])}\")\n",
    "\n",
    "\n",
    "    response_times = []  # Initialize response_times if not already\n",
    "    nltk_treebank_tokenizer = TreebankWordTokenizer()\n",
    "    # Step 3: Iterate over the sampled data\n",
    "    for example in tqdm(sampled_dataset['data'], desc=\"Processing examples\"):\n",
    "        # Extract the natural language description and reference code\n",
    "        text_input = example.get('nl') or example.get('question') or example.get('text')\n",
    "        reference_code = example.get('code') or example.get('solutions') or example.get('answer')\n",
    "\n",
    "        if not text_input or not reference_code:\n",
    "            no_text_or_code += 1\n",
    "            continue  # Skip if required fields are missing\n",
    "\n",
    "        # Prepare the prompt\n",
    "        prompt = f\"Write code for the following description:\\n{text_input}\"\n",
    "\n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "        # Generate code using the model\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            output_sequences = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs.get('attention_mask', None),\n",
    "                max_length=inputs['input_ids'].shape[1] + 256,  # Adjust max_length as needed\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            response_times.append(response_time)\n",
    "\n",
    "            # Decode the generated code\n",
    "            generated_code = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            # Remove the prompt from the generated code\n",
    "            generated_code = generated_code[len(prompt):].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Inference error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Update counters\n",
    "        total_examples += 1\n",
    "\n",
    "        # Check for exact match\n",
    "        is_exact_match = generated_code.strip() == reference_code.strip()\n",
    "        if is_exact_match:\n",
    "            exact_matches += 1\n",
    "\n",
    "        # Tokenize the reference and generated code\n",
    "        reference_tokens = nltk_treebank_tokenizer.tokenize(reference_code)\n",
    "        candidate_tokens = nltk_treebank_tokenizer.tokenize(generated_code)\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "        # Store the results\n",
    "        results.append({\n",
    "            'text_input': text_input,\n",
    "            'reference_code': reference_code,\n",
    "            'generated_code': generated_code,\n",
    "            'is_exact_match': is_exact_match,  # Include exact match result\n",
    "            'bleu_score': bleu_score,\n",
    "            'response_time': response_time\n",
    "        })\n",
    "\n",
    "    print(f\"Processed {total_examples} examples.\")\n",
    "    print(f\"no_text_or_code: {no_text_or_code}\")\n",
    "\n",
    "    # Assuming bleu_scores is a list of BLEU scores\n",
    "    if bleu_scores:\n",
    "        # Calculate the average BLEU score\n",
    "        average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "        print(f\"\\nAverage BLEU score: {average_bleu:.4f}\")\n",
    "        \n",
    "        # Calculate percentiles\n",
    "        percentiles = [25, 50, 75, 90, 95, 99]\n",
    "        percentile_values = np.percentile(bleu_scores, percentiles)\n",
    "        \n",
    "        print(\"\\nBLEU Score Percentiles:\")\n",
    "        for p, value in zip(percentiles, percentile_values):\n",
    "            print(f\"{p}th percentile: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"\\nNo BLEU scores were calculated.\")\n",
    "\n",
    "\n",
    "    if response_times:\n",
    "        average_time = sum(response_times) / len(response_times)\n",
    "        total_time = sum(response_times)\n",
    "        max_time = max(response_times)\n",
    "        min_time = min(response_times)\n",
    "        throughput = total_examples / total_time if total_time > 0 else 0\n",
    "        print(f\"Average API response time: {average_time:.2f} seconds\")\n",
    "        print(f\"Total API response time: {total_time:.2f} seconds\")\n",
    "        print(f\"Max API response time: {max_time:.2f} seconds\")\n",
    "        print(f\"Min API response time: {min_time:.2f} seconds\")\n",
    "        print(f\"Throughput: {throughput:.2f} requests per second\")\n",
    "    else:\n",
    "        print(\"\\nNo response times were recorded.\")\n",
    "\n",
    "    if total_examples > 0:\n",
    "        exact_match_rate = exact_matches / total_examples * 100\n",
    "        print(f\"Exact match rate: {exact_match_rate:.2f}% ({exact_matches}/{total_examples})\")\n",
    "    else:\n",
    "        print(\"\\nNo examples were processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 93847/93847 [00:00<00:00, 1665994.47 examples/s]\n",
      "Generating test split: 100%|██████████| 8118/8118 [00:00<00:00, 2333426.53 examples/s]\n",
      "Generating validation split: 100%|██████████| 4432/4432 [00:00<00:00, 2322773.38 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dataset at position 0 has at least one split: ['train', 'test', 'validation']\nPlease pick one to interleave with the other datasets, for example: dataset['train']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#dataset_name=['codeparrot/apps'\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodeparrot/xlcost-text-to-code\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m sampled_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_sample_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m sampled_dataset \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: sampled_dataset,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodeparrot/apps\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of sampled dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sampled_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[16], line 29\u001b[0m, in \u001b[0;36mload_and_sample_datasets\u001b[0;34m(fraction, dataset_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo datasets were loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Combine datasets\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m combined_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Sample 10% of the data\u001b[39;00m\n\u001b[1;32m     32\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(combined_dataset) \u001b[38;5;241m*\u001b[39m fraction)\n",
      "File \u001b[0;32m~/anaconda3/envs/text_to_code/lib/python3.12/site-packages/datasets/combine.py:197\u001b[0m, in \u001b[0;36mconcatenate_datasets\u001b[0;34m(dsets, info, split, axis)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset:\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a list of Dataset objects or a list of IterableDataset objects, but element at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis an empty dataset dictionary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has at least one split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pick one to interleave with the other datasets, for example: dataset[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dataset))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m         )\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a list of Dataset objects or a list of IterableDataset objects, but element at position \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset at position 0 has at least one split: ['train', 'test', 'validation']\nPlease pick one to interleave with the other datasets, for example: dataset['train']"
     ]
    }
   ],
   "source": [
    "model_names = ['Salesforce/codegen-350M-mono','codeparrot/codeparrot-small']\n",
    "fraction=0.001\n",
    "#dataset_name=['codeparrot/apps'\n",
    "dataset_name = 'codeparrot/xlcost-text-to-code'\n",
    "\n",
    "\n",
    "sampled_dataset = load_and_sample_datasets(fraction=fraction, dataset_name=dataset_name)\n",
    "sampled_dataset = {\n",
    "    'data': sampled_dataset,\n",
    "    'name': 'codeparrot/apps'\n",
    "}\n",
    "\n",
    "print(\"length of sampled dataset\", len(sampled_dataset['data']))\n",
    "for model_name in model_names:\n",
    "    load_model_and_tokenizer(model_name, sampled_dataset=sampled_dataset)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_to_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
